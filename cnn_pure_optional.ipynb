{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关函数库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reset default graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "y = tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define variables and compute processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 1\n",
    "\n",
    "conv1_size = 3\n",
    "conv1_deep = 32\n",
    "\n",
    "conv2_size = 3\n",
    "conv2_deep = 64\n",
    "\n",
    "fc1_nodes = 128\n",
    "num_classes = 10\n",
    "\n",
    "with tf.variable_scope('layer1-conv1'):\n",
    "    conv1_weights = tf.get_variable('weights',[conv1_size, conv1_size, num_channels, conv1_deep],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv1_biases = tf.get_variable('biases',[conv1_deep],initializer=tf.zeros_initializer)\n",
    "    conv1 = tf.nn.conv2d(x,conv1_weights,strides=[1,2,2,1],padding='SAME')         ##############################################\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1,conv1_biases))\n",
    "\n",
    "with tf.variable_scope('layer2-pool1'):\n",
    "    pool1 = tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "with tf.variable_scope('layer3-conv2'):\n",
    "    conv2_weights = tf.get_variable('weights',[conv2_size,conv2_size,conv1_deep,conv2_deep],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv2_biases = tf.get_variable('biases',[conv2_deep],initializer=tf.zeros_initializer)\n",
    "    conv2 = tf.nn.conv2d(pool1,conv2_weights,strides=[1,2,2,1],padding='SAME')\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases))\n",
    "\n",
    "with tf.variable_scope('layer4-pool2'):\n",
    "    pool2 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2_shape = pool2.get_shape().as_list()\n",
    "# from functools import reduce\n",
    "# total_nodes = reduce(tf.multiply,pool2_shape[1:])\n",
    "total_nodes = pool2_shape[1]*pool2_shape[2]*pool2_shape[3]\n",
    "pool2_flat = tf.reshape(pool2,(-1,total_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 2, 2, 64]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to reshape raw data\n",
    "def data_reshape(data):\n",
    "    return np.reshape(data,[-1,28,28,1])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_x, train_y = mnist.train.next_batch(100)\n",
    "a,b = sess.run([pool2, pool2_flat],feed_dict={x:data_reshape(train_x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer5-fc1'):\n",
    "    fc1_weights = tf.get_variable('weights', [256, 128],initializer=tf.truncated_normal_initializer)\n",
    "    fc1_biases = tf.get_variable('biases',[128],initializer=tf.zeros_initializer)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 1\n",
    "\n",
    "conv1_size = 3\n",
    "conv1_deep = 32\n",
    "\n",
    "conv2_size = 3\n",
    "conv2_deep = 64\n",
    "\n",
    "fc1_nodes = 128\n",
    "num_classes = 10\n",
    "\n",
    "with tf.variable_scope('layer1-conv1'):\n",
    "    conv1_weights = tf.get_variable('weights',[conv1_size, conv1_size, num_channels, conv1_deep],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv1_biases = tf.get_variable('biases',[conv1_deep],initializer=tf.zeros_initializer)\n",
    "    conv1 = tf.nn.conv2d(x,conv1_weights,strides=[1,2,2,1],padding='SAME')         ##############################################\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1,conv1_biases))\n",
    "\n",
    "with tf.variable_scope('layer2-pool1'):\n",
    "    pool1 = tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "with tf.variable_scope('layer3-conv2'):\n",
    "    conv2_weights = tf.get_variable('weights',[conv2_size,conv2_size,conv1_deep,conv2_deep],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv2_biases = tf.get_variable('biases',[conv2_deep],initializer=tf.zeros_initializer)\n",
    "    conv2 = tf.nn.conv2d(pool1,conv2_weights,strides=[1,2,2,1],padding='SAME')\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases))\n",
    "\n",
    "with tf.variable_scope('layer4-pool2'):\n",
    "    pool2 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "pool2_shape = pool2.get_shape().as_list()\n",
    "# from functools import reduce\n",
    "# total_nodes = tf.map_fn(tf.multiply,pool2_shape[1:])\n",
    "total_nodes = pool2_shape[1]*pool2_shape[2]*pool2_shape[3]\n",
    "total_nodes = int(total_nodes)\n",
    "pool2_flat = tf.reshape(pool2,(-1,total_nodes))\n",
    "\n",
    "with tf.variable_scope('layer5-fc1'):\n",
    "    fc1_weights = tf.get_variable('weights', [total_nodes, fc1_nodes],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fc1_biases = tf.get_variable('biases',[fc1_nodes],initializer=tf.zeros_initializer)\n",
    "    fc1 = tf.matmul(pool2_flat,fc1_weights)+fc1_biases\n",
    "    relu3 = tf.nn.relu(fc1)\n",
    "\n",
    "with tf.variable_scope('layer6-output'):\n",
    "    output_weights = tf.get_variable('weights',[fc1_nodes,num_classes],initializer=tf.truncated_normal_initializer)\n",
    "    output_biases = tf.get_variable('biases',[num_classes],initializer=tf.zeros_initializer)\n",
    "    logit = tf.matmul(relu3,output_weights)+output_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "y_ = logit\n",
    "\n",
    "# define loss function\n",
    "scores = tf.nn.softmax(y_)\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(tf.clip_by_value(scores,1e-30,1.0)),1)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "learning_rate = 0.01\n",
    "global_step = tf.Variable(1,trainable=False)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 epochs, loss on training data is 3.5264127254486084\n",
      "After 20 epochs, loss on training data is 3.53157639503479\n",
      "After 30 epochs, loss on training data is 3.555483102798462\n",
      "After 40 epochs, loss on training data is 3.4738688468933105\n",
      "After 50 epochs, loss on training data is 3.3952488899230957\n",
      "After 60 epochs, loss on training data is 3.3361055850982666\n",
      "After 70 epochs, loss on training data is 3.2544901371002197\n",
      "After 80 epochs, loss on training data is 3.281383752822876\n",
      "After 90 epochs, loss on training data is 3.409984827041626\n",
      "After 100 epochs, loss on training data is 3.1839346885681152\n",
      "After 110 epochs, loss on training data is 3.140779972076416\n",
      "After 120 epochs, loss on training data is 3.1322970390319824\n",
      "After 130 epochs, loss on training data is 2.9142425060272217\n",
      "After 140 epochs, loss on training data is 2.999919891357422\n",
      "After 150 epochs, loss on training data is 3.0238242149353027\n",
      "After 160 epochs, loss on training data is 2.9819445610046387\n",
      "After 170 epochs, loss on training data is 2.98575496673584\n",
      "After 180 epochs, loss on training data is 2.865295886993408\n",
      "After 190 epochs, loss on training data is 2.8292295932769775\n",
      "After 200 epochs, loss on training data is 2.8815808296203613\n",
      "After 210 epochs, loss on training data is 2.874380588531494\n",
      "After 220 epochs, loss on training data is 2.7814526557922363\n",
      "After 230 epochs, loss on training data is 2.770522356033325\n",
      "After 240 epochs, loss on training data is 2.79582142829895\n",
      "After 250 epochs, loss on training data is 2.6516101360321045\n",
      "After 260 epochs, loss on training data is 2.6774587631225586\n",
      "After 270 epochs, loss on training data is 2.707432985305786\n",
      "After 280 epochs, loss on training data is 2.650113344192505\n",
      "After 290 epochs, loss on training data is 2.609678268432617\n",
      "After 300 epochs, loss on training data is 2.656386137008667\n",
      "After 310 epochs, loss on training data is 2.508838415145874\n",
      "After 320 epochs, loss on training data is 2.5221378803253174\n",
      "After 330 epochs, loss on training data is 2.552277088165283\n",
      "After 340 epochs, loss on training data is 2.415886878967285\n",
      "After 350 epochs, loss on training data is 2.4633126258850098\n",
      "After 360 epochs, loss on training data is 2.4466733932495117\n",
      "After 370 epochs, loss on training data is 2.516967296600342\n",
      "After 380 epochs, loss on training data is 2.4770262241363525\n",
      "After 390 epochs, loss on training data is 2.4183170795440674\n",
      "After 400 epochs, loss on training data is 2.4327497482299805\n",
      "After 410 epochs, loss on training data is 2.4264063835144043\n",
      "After 420 epochs, loss on training data is 2.3895742893218994\n",
      "After 430 epochs, loss on training data is 2.3532629013061523\n",
      "After 440 epochs, loss on training data is 2.3675239086151123\n",
      "After 450 epochs, loss on training data is 2.3745508193969727\n",
      "After 460 epochs, loss on training data is 2.40792179107666\n",
      "After 470 epochs, loss on training data is 2.348590135574341\n",
      "After 480 epochs, loss on training data is 2.337045192718506\n",
      "After 490 epochs, loss on training data is 2.2943453788757324\n",
      "After 500 epochs, loss on training data is 2.309419870376587\n",
      "After 510 epochs, loss on training data is 2.3335180282592773\n",
      "After 520 epochs, loss on training data is 2.2970662117004395\n",
      "After 530 epochs, loss on training data is 2.280726432800293\n",
      "After 540 epochs, loss on training data is 2.267911434173584\n",
      "After 550 epochs, loss on training data is 2.3019754886627197\n",
      "After 560 epochs, loss on training data is 2.179044008255005\n",
      "After 570 epochs, loss on training data is 2.273684024810791\n",
      "After 580 epochs, loss on training data is 2.2470364570617676\n",
      "After 590 epochs, loss on training data is 2.227395534515381\n",
      "After 600 epochs, loss on training data is 2.1583988666534424\n",
      "After 610 epochs, loss on training data is 2.1550896167755127\n",
      "After 620 epochs, loss on training data is 2.1534314155578613\n",
      "After 630 epochs, loss on training data is 2.1713707447052\n",
      "After 640 epochs, loss on training data is 2.2032487392425537\n",
      "After 650 epochs, loss on training data is 2.1292572021484375\n",
      "After 660 epochs, loss on training data is 2.185701370239258\n",
      "After 670 epochs, loss on training data is 2.1433334350585938\n",
      "After 680 epochs, loss on training data is 2.166682004928589\n",
      "After 690 epochs, loss on training data is 2.12199068069458\n",
      "After 700 epochs, loss on training data is 2.1201469898223877\n",
      "After 710 epochs, loss on training data is 2.1365840435028076\n",
      "After 720 epochs, loss on training data is 2.1603140830993652\n",
      "After 730 epochs, loss on training data is 2.073965311050415\n",
      "After 740 epochs, loss on training data is 2.054610252380371\n",
      "After 750 epochs, loss on training data is 2.0963056087493896\n",
      "After 760 epochs, loss on training data is 2.0705273151397705\n",
      "After 770 epochs, loss on training data is 2.0682690143585205\n",
      "After 780 epochs, loss on training data is 2.0848584175109863\n",
      "After 790 epochs, loss on training data is 2.0448150634765625\n",
      "After 800 epochs, loss on training data is 2.0249288082122803\n",
      "After 810 epochs, loss on training data is 2.015406608581543\n",
      "After 820 epochs, loss on training data is 1.9573278427124023\n",
      "After 830 epochs, loss on training data is 1.9729490280151367\n",
      "After 840 epochs, loss on training data is 2.0114853382110596\n",
      "After 850 epochs, loss on training data is 1.998366117477417\n",
      "After 860 epochs, loss on training data is 1.9338862895965576\n",
      "After 870 epochs, loss on training data is 1.980120062828064\n",
      "After 880 epochs, loss on training data is 1.9265165328979492\n",
      "After 890 epochs, loss on training data is 1.9638053178787231\n",
      "After 900 epochs, loss on training data is 1.9953714609146118\n",
      "After 910 epochs, loss on training data is 1.9029268026351929\n",
      "After 920 epochs, loss on training data is 1.941470742225647\n",
      "After 930 epochs, loss on training data is 1.8972092866897583\n",
      "After 940 epochs, loss on training data is 1.847569227218628\n",
      "After 950 epochs, loss on training data is 1.9099550247192383\n",
      "After 960 epochs, loss on training data is 1.845615267753601\n",
      "After 970 epochs, loss on training data is 1.9370381832122803\n",
      "After 980 epochs, loss on training data is 1.9018399715423584\n",
      "After 990 epochs, loss on training data is 1.904148817062378\n",
      "After 1000 epochs, loss on training data is 1.8896071910858154\n",
      "After 1010 epochs, loss on training data is 1.8864936828613281\n",
      "After 1020 epochs, loss on training data is 1.8618829250335693\n",
      "After 1030 epochs, loss on training data is 1.8603581190109253\n",
      "After 1040 epochs, loss on training data is 1.8851401805877686\n",
      "After 1050 epochs, loss on training data is 1.8632922172546387\n",
      "After 1060 epochs, loss on training data is 1.9162321090698242\n",
      "After 1070 epochs, loss on training data is 1.9055753946304321\n",
      "After 1080 epochs, loss on training data is 1.8702374696731567\n",
      "After 1090 epochs, loss on training data is 1.8268680572509766\n",
      "After 1100 epochs, loss on training data is 1.8556993007659912\n",
      "After 1110 epochs, loss on training data is 1.8180233240127563\n",
      "After 1120 epochs, loss on training data is 1.7199389934539795\n",
      "After 1130 epochs, loss on training data is 1.7675890922546387\n",
      "After 1140 epochs, loss on training data is 1.8052759170532227\n",
      "After 1150 epochs, loss on training data is 1.80872642993927\n",
      "After 1160 epochs, loss on training data is 1.757727026939392\n",
      "After 1170 epochs, loss on training data is 1.7514034509658813\n",
      "After 1180 epochs, loss on training data is 1.7574820518493652\n",
      "After 1190 epochs, loss on training data is 1.761738657951355\n",
      "After 1200 epochs, loss on training data is 1.753186821937561\n",
      "After 1210 epochs, loss on training data is 1.7290291786193848\n",
      "After 1220 epochs, loss on training data is 1.7075247764587402\n",
      "After 1230 epochs, loss on training data is 1.6888091564178467\n",
      "After 1240 epochs, loss on training data is 1.709135890007019\n",
      "After 1250 epochs, loss on training data is 1.7450803518295288\n",
      "After 1260 epochs, loss on training data is 1.6044549942016602\n",
      "After 1270 epochs, loss on training data is 1.6051348447799683\n",
      "After 1280 epochs, loss on training data is 1.6567094326019287\n",
      "After 1290 epochs, loss on training data is 1.6732338666915894\n",
      "After 1300 epochs, loss on training data is 1.6881107091903687\n",
      "After 1310 epochs, loss on training data is 1.7296545505523682\n",
      "After 1320 epochs, loss on training data is 1.5940324068069458\n",
      "After 1330 epochs, loss on training data is 1.6279460191726685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1340 epochs, loss on training data is 1.6669485569000244\n",
      "After 1350 epochs, loss on training data is 1.6188722848892212\n",
      "After 1360 epochs, loss on training data is 1.6501352787017822\n",
      "After 1370 epochs, loss on training data is 1.543147325515747\n",
      "After 1380 epochs, loss on training data is 1.6404683589935303\n",
      "After 1390 epochs, loss on training data is 1.5744253396987915\n",
      "After 1400 epochs, loss on training data is 1.5849460363388062\n",
      "After 1410 epochs, loss on training data is 1.546904444694519\n",
      "After 1420 epochs, loss on training data is 1.6408504247665405\n",
      "After 1430 epochs, loss on training data is 1.6216599941253662\n",
      "After 1440 epochs, loss on training data is 1.603669285774231\n",
      "After 1450 epochs, loss on training data is 1.5952630043029785\n",
      "After 1460 epochs, loss on training data is 1.5951167345046997\n",
      "After 1470 epochs, loss on training data is 1.5173288583755493\n",
      "After 1480 epochs, loss on training data is 1.5389010906219482\n",
      "After 1490 epochs, loss on training data is 1.582766056060791\n",
      "After 1500 epochs, loss on training data is 1.4423258304595947\n",
      "After 1510 epochs, loss on training data is 1.5812792778015137\n",
      "After 1520 epochs, loss on training data is 1.5103423595428467\n",
      "After 1530 epochs, loss on training data is 1.5722203254699707\n",
      "After 1540 epochs, loss on training data is 1.537572979927063\n",
      "After 1550 epochs, loss on training data is 1.4959213733673096\n",
      "After 1560 epochs, loss on training data is 1.4610422849655151\n",
      "After 1570 epochs, loss on training data is 1.5484607219696045\n",
      "After 1580 epochs, loss on training data is 1.4713900089263916\n",
      "After 1590 epochs, loss on training data is 1.5437043905258179\n",
      "After 1600 epochs, loss on training data is 1.4502978324890137\n",
      "After 1610 epochs, loss on training data is 1.424836277961731\n",
      "After 1620 epochs, loss on training data is 1.4674502611160278\n",
      "After 1630 epochs, loss on training data is 1.5104732513427734\n",
      "After 1640 epochs, loss on training data is 1.429357647895813\n",
      "After 1650 epochs, loss on training data is 1.44181227684021\n",
      "After 1660 epochs, loss on training data is 1.3838002681732178\n",
      "After 1670 epochs, loss on training data is 1.437972903251648\n",
      "After 1680 epochs, loss on training data is 1.4475593566894531\n",
      "After 1690 epochs, loss on training data is 1.415709376335144\n",
      "After 1700 epochs, loss on training data is 1.4066290855407715\n",
      "After 1710 epochs, loss on training data is 1.352843165397644\n",
      "After 1720 epochs, loss on training data is 1.370940089225769\n",
      "After 1730 epochs, loss on training data is 1.3614919185638428\n",
      "After 1740 epochs, loss on training data is 1.418396234512329\n",
      "After 1750 epochs, loss on training data is 1.3820397853851318\n",
      "After 1760 epochs, loss on training data is 1.455987811088562\n",
      "After 1770 epochs, loss on training data is 1.382989525794983\n",
      "After 1780 epochs, loss on training data is 1.3921914100646973\n",
      "After 1790 epochs, loss on training data is 1.4165523052215576\n",
      "After 1800 epochs, loss on training data is 1.3377577066421509\n",
      "After 1810 epochs, loss on training data is 1.3813344240188599\n",
      "After 1820 epochs, loss on training data is 1.3938071727752686\n",
      "After 1830 epochs, loss on training data is 1.351462960243225\n",
      "After 1840 epochs, loss on training data is 1.398347020149231\n",
      "After 1850 epochs, loss on training data is 1.2974603176116943\n",
      "After 1860 epochs, loss on training data is 1.3362210988998413\n",
      "After 1870 epochs, loss on training data is 1.276036024093628\n",
      "After 1880 epochs, loss on training data is 1.2925262451171875\n",
      "After 1890 epochs, loss on training data is 1.3035892248153687\n",
      "After 1900 epochs, loss on training data is 1.3200558423995972\n",
      "After 1910 epochs, loss on training data is 1.3119672536849976\n",
      "After 1920 epochs, loss on training data is 1.3401169776916504\n",
      "After 1930 epochs, loss on training data is 1.188270092010498\n",
      "After 1940 epochs, loss on training data is 1.3211736679077148\n",
      "After 1950 epochs, loss on training data is 1.3172109127044678\n",
      "After 1960 epochs, loss on training data is 1.260280728340149\n",
      "After 1970 epochs, loss on training data is 1.2673343420028687\n",
      "After 1980 epochs, loss on training data is 1.2175809144973755\n",
      "After 1990 epochs, loss on training data is 1.2444006204605103\n",
      "After 2000 epochs, loss on training data is 1.287528395652771\n",
      "After 2010 epochs, loss on training data is 1.2377197742462158\n",
      "After 2020 epochs, loss on training data is 1.2623525857925415\n",
      "After 2030 epochs, loss on training data is 1.2483857870101929\n",
      "After 2040 epochs, loss on training data is 1.2234503030776978\n",
      "After 2050 epochs, loss on training data is 1.2642035484313965\n",
      "After 2060 epochs, loss on training data is 1.2777540683746338\n",
      "After 2070 epochs, loss on training data is 1.137853980064392\n",
      "After 2080 epochs, loss on training data is 1.1794707775115967\n",
      "After 2090 epochs, loss on training data is 1.2166550159454346\n",
      "After 2100 epochs, loss on training data is 1.208682894706726\n",
      "After 2110 epochs, loss on training data is 1.2248133420944214\n",
      "After 2120 epochs, loss on training data is 1.1647182703018188\n",
      "After 2130 epochs, loss on training data is 1.153335690498352\n",
      "After 2140 epochs, loss on training data is 1.25271475315094\n",
      "After 2150 epochs, loss on training data is 1.2226954698562622\n",
      "After 2160 epochs, loss on training data is 1.2481197118759155\n",
      "After 2170 epochs, loss on training data is 1.0966402292251587\n",
      "After 2180 epochs, loss on training data is 1.2234396934509277\n",
      "After 2190 epochs, loss on training data is 1.1970220804214478\n",
      "After 2200 epochs, loss on training data is 1.1281572580337524\n",
      "After 2210 epochs, loss on training data is 1.1725565195083618\n",
      "After 2220 epochs, loss on training data is 1.1126922369003296\n",
      "After 2230 epochs, loss on training data is 1.0993977785110474\n",
      "After 2240 epochs, loss on training data is 1.130889654159546\n",
      "After 2250 epochs, loss on training data is 1.1253535747528076\n",
      "After 2260 epochs, loss on training data is 1.1634857654571533\n",
      "After 2270 epochs, loss on training data is 1.1678401231765747\n",
      "After 2280 epochs, loss on training data is 1.179220199584961\n",
      "After 2290 epochs, loss on training data is 1.1094118356704712\n",
      "After 2300 epochs, loss on training data is 1.1963140964508057\n",
      "After 2310 epochs, loss on training data is 1.1409547328948975\n",
      "After 2320 epochs, loss on training data is 1.1883893013000488\n",
      "After 2330 epochs, loss on training data is 1.2092726230621338\n",
      "After 2340 epochs, loss on training data is 1.1319448947906494\n",
      "After 2350 epochs, loss on training data is 1.1209362745285034\n",
      "After 2360 epochs, loss on training data is 1.13446044921875\n",
      "After 2370 epochs, loss on training data is 1.1862479448318481\n",
      "After 2380 epochs, loss on training data is 1.0489001274108887\n",
      "After 2390 epochs, loss on training data is 1.0647023916244507\n",
      "After 2400 epochs, loss on training data is 1.0251305103302002\n",
      "After 2410 epochs, loss on training data is 1.1315343379974365\n",
      "After 2420 epochs, loss on training data is 1.1968005895614624\n",
      "After 2430 epochs, loss on training data is 1.061689853668213\n",
      "After 2440 epochs, loss on training data is 1.1278167963027954\n",
      "After 2450 epochs, loss on training data is 1.095636248588562\n",
      "After 2460 epochs, loss on training data is 1.1138207912445068\n",
      "After 2470 epochs, loss on training data is 1.0681103467941284\n",
      "After 2480 epochs, loss on training data is 1.1451466083526611\n",
      "After 2490 epochs, loss on training data is 1.062312126159668\n",
      "After 2500 epochs, loss on training data is 1.0063306093215942\n",
      "After 2510 epochs, loss on training data is 0.9728501439094543\n",
      "After 2520 epochs, loss on training data is 1.02430260181427\n",
      "After 2530 epochs, loss on training data is 1.0277128219604492\n",
      "After 2540 epochs, loss on training data is 1.1059322357177734\n",
      "After 2550 epochs, loss on training data is 0.9966366291046143\n",
      "After 2560 epochs, loss on training data is 1.066318392753601\n",
      "After 2570 epochs, loss on training data is 1.010666012763977\n",
      "After 2580 epochs, loss on training data is 1.0074352025985718\n",
      "After 2590 epochs, loss on training data is 0.9542006254196167\n",
      "After 2600 epochs, loss on training data is 1.0612435340881348\n",
      "After 2610 epochs, loss on training data is 1.0389028787612915\n",
      "After 2620 epochs, loss on training data is 0.9811106324195862\n",
      "After 2630 epochs, loss on training data is 0.9407004714012146\n",
      "After 2640 epochs, loss on training data is 0.9617840647697449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2650 epochs, loss on training data is 1.0149942636489868\n",
      "After 2660 epochs, loss on training data is 1.0123474597930908\n",
      "After 2670 epochs, loss on training data is 1.0233858823776245\n",
      "After 2680 epochs, loss on training data is 0.9585253000259399\n",
      "After 2690 epochs, loss on training data is 0.9694441556930542\n",
      "After 2700 epochs, loss on training data is 1.0091521739959717\n",
      "After 2710 epochs, loss on training data is 0.9783036112785339\n",
      "After 2720 epochs, loss on training data is 1.042864441871643\n",
      "After 2730 epochs, loss on training data is 1.0542993545532227\n",
      "After 2740 epochs, loss on training data is 0.9993411302566528\n",
      "After 2750 epochs, loss on training data is 0.9917824268341064\n",
      "After 2760 epochs, loss on training data is 0.9093539714813232\n",
      "After 2770 epochs, loss on training data is 0.9581583738327026\n",
      "After 2780 epochs, loss on training data is 0.9498676657676697\n",
      "After 2790 epochs, loss on training data is 0.9217634797096252\n",
      "After 2800 epochs, loss on training data is 0.9241556525230408\n",
      "After 2810 epochs, loss on training data is 0.9519103169441223\n",
      "After 2820 epochs, loss on training data is 1.0251352787017822\n",
      "After 2830 epochs, loss on training data is 0.9855939745903015\n",
      "After 2840 epochs, loss on training data is 0.8830127120018005\n",
      "After 2850 epochs, loss on training data is 0.9044238328933716\n",
      "After 2860 epochs, loss on training data is 1.0290087461471558\n",
      "After 2870 epochs, loss on training data is 0.8174074292182922\n",
      "After 2880 epochs, loss on training data is 0.9461840987205505\n",
      "After 2890 epochs, loss on training data is 0.9092085957527161\n",
      "After 2900 epochs, loss on training data is 0.9308982491493225\n",
      "After 2910 epochs, loss on training data is 0.9182700514793396\n",
      "After 2920 epochs, loss on training data is 0.9142604470252991\n",
      "After 2930 epochs, loss on training data is 0.8643072247505188\n",
      "After 2940 epochs, loss on training data is 0.9644097089767456\n",
      "After 2950 epochs, loss on training data is 0.868541955947876\n",
      "After 2960 epochs, loss on training data is 0.9213041067123413\n",
      "After 2970 epochs, loss on training data is 0.8439831137657166\n",
      "After 2980 epochs, loss on training data is 0.8688016533851624\n",
      "After 2990 epochs, loss on training data is 0.8945748805999756\n",
      "After 3000 epochs, loss on training data is 0.8926900029182434\n"
     ]
    }
   ],
   "source": [
    "# define a function to reshape raw data\n",
    "def data_reshape(data):\n",
    "    return np.reshape(data,[-1,28,28,1])\n",
    "\n",
    "# initializing all variables and training model\n",
    "batch_size = 300\n",
    "training_epochs = 3000\n",
    "display_epoch = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        train_x, train_y = mnist.train.next_batch(batch_size)\n",
    "        _,training_loss = sess.run([optimizer,loss],feed_dict={x:data_reshape(train_x),y:train_y})\n",
    "\n",
    "        if (epoch+1) % display_epoch == 0:\n",
    "            print('After {} epochs, loss on training data is {}'.format(epoch+1,training_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
